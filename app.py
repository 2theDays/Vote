"""
ì„ ê±° ì „ëµ ì¸ì‚¬ì´íŠ¸: ì˜ˆì¸¡ê³¼ ì „ë§
================================
ë„¤ì´ë²„ + êµ¬ê¸€(Apify) íŠ¸ë Œë“œ ì¢…í•© ë¶„ì„ ì‹œìŠ¤í…œ
"""

import streamlit as st
import google.generativeai as genai
from duckduckgo_search import DDGS
import requests
import time
import json
import random
import re
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
import logging
import sqlite3
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import wraps
from typing import Dict, List, Optional, Any, Tuple
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì„ ê±° ì „ëµ: ì˜ˆì¸¡ê³¼ ì „ë§",
    layout="wide",
    page_icon="ğŸ—³ï¸",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
/* ë©”ì¸ í…Œë§ˆ */
.main { background-color: #0f172a; color: #e2e8f0; }
h1, h2, h3 { color: #f1f5f9; }

/* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
.stButton>button {
    background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%);
    color: white;
    border-radius: 12px;
    height: 50px;
    font-size: 16px;
    font-weight: 600;
    border: none;
    transition: all 0.3s ease;
}
.stButton>button:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 12px rgba(59, 130, 246, 0.4);
}

/* ì¹´ë“œ ìŠ¤íƒ€ì¼ */
.candidate-card {
    background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%);
    border-radius: 16px;
    padding: 20px;
    margin: 10px 0;
    border: 1px solid #334155;
}

/* ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ */
div[data-testid="metric-container"] {
    background: #1e293b;
    border-radius: 10px;
    padding: 15px;
    border: 1px solid #334155;
}

/* íƒ­ ìŠ¤íƒ€ì¼ */
.stTabs [data-baseweb="tab-list"] {
    gap: 8px;
}
.stTabs [data-baseweb="tab"] {
    background: #1e293b;
    border-radius: 8px;
    color: #94a3b8;
}
.stTabs [aria-selected="true"] {
    background: #3b82f6 !important;
    color: white !important;
}

/* í”„ë¡œê·¸ë ˆìŠ¤ ë°” */
.stProgress > div > div {
    background: linear-gradient(90deg, #3b82f6, #8b5cf6);
}

/* ì‚¬ì´ë“œë°” */
section[data-testid="stSidebar"] {
    background: #0f172a;
    border-right: 1px solid #334155;
}

/* ë°°ì§€ ìŠ¤íƒ€ì¼ */
.party-badge {
    display: inline-block;
    padding: 4px 12px;
    border-radius: 20px;
    font-size: 0.85em;
    font-weight: 600;
    margin-left: 10px;
}
.trend-badge {
    display: inline-block;
    padding: 3px 10px;
    border-radius: 15px;
    font-size: 0.75em;
    margin-left: 8px;
}
</style>
""", unsafe_allow_html=True)

# ==========================================
# ì„¤ì • ìƒìˆ˜
# ==========================================
CONFIG = {
    "MAX_CANDIDATES": 10,
    "TREND_DAYS": 180,
    "PREDICTION_DAYS": 30,
    "MAX_NEWS": 30,
    "TIMEOUT": 30,
    "MAX_RETRIES": 3,
    "RETRY_DELAY": 2,
    "PARALLEL_WORKERS": 5,
    "COLORS": ['#3b82f6', '#ef4444', '#22c55e', '#f59e0b', '#8b5cf6', '#ec4899', '#06b6d4', '#84cc16'],
    "PARTY_COLORS": {
        "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹": "#1d4ed8",
        "ë¯¼ì£¼ë‹¹": "#1d4ed8",
        "êµ­ë¯¼ì˜í˜": "#b91c1c",
        "ê°œí˜ì‹ ë‹¹": "#f97316",
        "ì •ì˜ë‹¹": "#eab308",
        "ë¬´ì†Œì†": "#6b7280",
        "ê¸°ë³¸": "#4b5563"
    }
}

# ==========================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ==========================================
def retry_on_failure(max_retries: int = 3, delay: float = 1.0):
    """ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    logger.warning(f"{func.__name__} ì‹¤íŒ¨ ({attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        time.sleep(delay * (attempt + 1))
            logger.error(f"{func.__name__} ìµœì¢… ì‹¤íŒ¨: {last_exception}")
            return None
        return wrapper
    return decorator


def generate_cache_key(data: Any) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    return hashlib.md5(str(data).encode()).hexdigest()[:16]


def safe_get_value(df: pd.DataFrame, col: str, idx: int = -1) -> Optional[float]:
    """ì•ˆì „í•œ ë°ì´í„°í”„ë ˆì„ ê°’ ì¶”ì¶œ"""
    if df is None or df.empty or col not in df.columns:
        return None
    try:
        return float(df[col].iloc[idx])
    except (IndexError, ValueError, TypeError):
        return None


def clean_json(text: str) -> Optional[Dict]:
    """JSON í…ìŠ¤íŠ¸ íŒŒì‹±"""
    if not text:
        return None
    try:
        text = re.sub(r'```json\s*|```\s*', '', text).strip()
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
        if match:
            return json.loads(match.group(0).replace('\n', ' '))
        return json.loads(text)
    except json.JSONDecodeError as e:
        logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None


def contains_name(text: str, name: str) -> bool:
    """í…ìŠ¤íŠ¸ì— ì´ë¦„ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    if not text or not name:
        return False
    return name in text or (len(name) >= 2 and name[1:] in text)


def get_party_color(party: str) -> str:
    """ì •ë‹¹ ìƒ‰ìƒ ë°˜í™˜"""
    if not party:
        return CONFIG["PARTY_COLORS"]["ê¸°ë³¸"]
    for key, color in CONFIG["PARTY_COLORS"].items():
        if key in party:
            return color
    return CONFIG["PARTY_COLORS"]["ê¸°ë³¸"]


# ==========================================
# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
# ==========================================
class DatabaseManager:
    """ë¶„ì„ íˆìŠ¤í† ë¦¬ ë° ê²°ê³¼ ì €ì¥ ê´€ë¦¬"""

    def __init__(self, db_path: str = "election_history.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS analysis_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    election_name TEXT NOT NULL,
                    candidates TEXT NOT NULL,
                    analysis_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    results TEXT,
                    trends_summary TEXT
                )
            ''')
            conn.execute('''
                CREATE TABLE IF NOT EXISTS candidate_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    analysis_id INTEGER,
                    candidate_name TEXT NOT NULL,
                    party TEXT,
                    current_role TEXT,
                    past_career TEXT,
                    poll_est INTEGER,
                    analysis TEXT,
                    sns_sentiment TEXT,
                    keywords TEXT,
                    naver_current REAL,
                    naver_predicted REAL,
                    google_current REAL,
                    google_predicted REAL,
                    FOREIGN KEY (analysis_id) REFERENCES analysis_history(id)
                )
            ''')
            conn.commit()

    def save_analysis(self, election: str, candidates: List[str],
                      results: List[Dict], trends: Dict) -> int:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # íŠ¸ë Œë“œ ìš”ì•½ ìƒì„±
            trends_summary = {}
            for source, df in trends.items():
                if not df.empty:
                    trends_summary[source] = {
                        col: {
                            "current": safe_get_value(df, col),
                            "mean": float(df[col].mean()) if col in df.columns else None
                        }
                        for col in df.columns
                    }

            cursor.execute('''
                INSERT INTO analysis_history (election_name, candidates, results, trends_summary)
                VALUES (?, ?, ?, ?)
            ''', (election, json.dumps(candidates), json.dumps(results), json.dumps(trends_summary)))

            analysis_id = cursor.lastrowid

            for result in results:
                cursor.execute('''
                    INSERT INTO candidate_data
                    (analysis_id, candidate_name, party, current_role, past_career,
                     poll_est, analysis, sns_sentiment, keywords)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    analysis_id,
                    result.get('name', ''),
                    result.get('party', ''),
                    result.get('current_role', ''),
                    result.get('past_career', ''),
                    result.get('poll_est', 0),
                    result.get('analysis', ''),
                    result.get('sns_sentiment', ''),
                    json.dumps(result.get('keywords', []))
                ))

            conn.commit()
            return analysis_id

    def get_history(self, limit: int = 10) -> List[Dict]:
        """ë¶„ì„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute('''
                SELECT id, election_name, candidates, analysis_date
                FROM analysis_history
                ORDER BY analysis_date DESC
                LIMIT ?
            ''', (limit,))
            return [dict(row) for row in cursor.fetchall()]

    def get_analysis_detail(self, analysis_id: int) -> Optional[Dict]:
        """íŠ¹ì • ë¶„ì„ ìƒì„¸ ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            cursor.execute('SELECT * FROM analysis_history WHERE id = ?', (analysis_id,))
            history = cursor.fetchone()

            if not history:
                return None

            cursor.execute('SELECT * FROM candidate_data WHERE analysis_id = ?', (analysis_id,))
            candidates = [dict(row) for row in cursor.fetchall()]

            return {
                "history": dict(history),
                "candidates": candidates
            }


# ==========================================
# API ë° ëª¨ë¸ ê´€ë¦¬
# ==========================================
def load_api_keys() -> Dict[str, Optional[str]]:
    """API í‚¤ ë¡œë“œ"""
    try:
        keys = {
            "gemini": st.secrets["GEMINI_API_KEY"],
            "naver_id": st.secrets["NAVER_CLIENT_ID"],
            "naver_secret": st.secrets["NAVER_CLIENT_SECRET"],
        }
        try:
            keys["apify"] = st.secrets["APIFY_API_KEY"]
        except KeyError:
            keys["apify"] = None
        return keys
    except Exception as e:
        logger.error(f"API í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.error("API í‚¤ ë¡œë“œ ì‹¤íŒ¨. `.streamlit/secrets.toml` íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()


@st.cache_data(ttl=3600)
def get_best_model(api_key: str) -> str:
    """ìµœì ì˜ Gemini ëª¨ë¸ ì„ íƒ"""
    genai.configure(api_key=api_key)
    try:
        models = [m.name for m in genai.list_models()
                  if 'generateContent' in m.supported_generation_methods]
        for m in models:
            if 'flash' in m.lower():
                return m
        if models:
            return models[0]
    except Exception as e:
        logger.warning(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    return "models/gemini-1.5-flash"


def validate_candidates(candidates: List[str]) -> Tuple[bool, str]:
    """í›„ë³´ì ëª©ë¡ ìœ íš¨ì„± ê²€ì‚¬"""
    if not candidates:
        return False, "í›„ë³´ìë¥¼ ìµœì†Œ 1ëª… ì…ë ¥í•˜ì„¸ìš”"
    if len(candidates) > CONFIG["MAX_CANDIDATES"]:
        return False, f"í›„ë³´ìëŠ” ìµœëŒ€ {CONFIG['MAX_CANDIDATES']}ëª…ê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤"
    for c in candidates:
        if len(c) < 2:
            return False, f"'{c}'ëŠ” ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 2ì)"
        if len(c) > 20:
            return False, f"'{c}'ëŠ” ë„ˆë¬´ ê¹ë‹ˆë‹¤ (ìµœëŒ€ 20ì)"
    return True, ""


# ==========================================
# íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘
# ==========================================
@st.cache_data(ttl=1800, show_spinner=False)
def get_naver_trend(candidates: List[str], n_id: str, n_secret: str) -> pd.DataFrame:
    """ë„¤ì´ë²„ ë°ì´í„°ë© íŠ¸ë Œë“œ ìˆ˜ì§‘"""
    try:
        url = "https://openapi.naver.com/v1/datalab/search"
        headers = {
            "X-Naver-Client-Id": n_id,
            "X-Naver-Client-Secret": n_secret,
            "Content-Type": "application/json"
        }
        end = datetime.now()
        start = end - timedelta(days=CONFIG["TREND_DAYS"])
        all_data = {}

        for i in range(0, len(candidates), 5):
            batch = candidates[i:i+5]
            body = {
                "startDate": start.strftime("%Y-%m-%d"),
                "endDate": end.strftime("%Y-%m-%d"),
                "timeUnit": "week",
                "keywordGroups": [{"groupName": c, "keywords": [c]} for c in batch]
            }

            resp = requests.post(url, headers=headers, json=body, timeout=CONFIG["TIMEOUT"])

            if resp.status_code == 200:
                for item in resp.json().get('results', []):
                    df = pd.DataFrame(item['data'])
                    if not df.empty:
                        df['period'] = pd.to_datetime(df['period'])
                        df.set_index('period', inplace=True)
                        all_data[item['title']] = df['ratio']
            else:
                logger.warning(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {resp.status_code}")

            time.sleep(0.3)

        return pd.DataFrame(all_data) if all_data else pd.DataFrame()

    except Exception as e:
        logger.error(f"ë„¤ì´ë²„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def get_google_trend_apify(candidates: List[str], api_key: str,
                           status_callback=None) -> pd.DataFrame:
    """Apify Google Trends ìˆ˜ì§‘"""
    if not api_key:
        return pd.DataFrame()

    try:
        run_url = f"https://api.apify.com/v2/acts/emastra~google-trends-scraper/runs?token={api_key}"
        run_input = {
            "searchTerms": candidates,
            "timeRange": "today 6-m",
            "geo": "KR",
            "isMultiple": True,
            "skipDebugScreen": True,
            "maxItems": 1
        }

        resp = requests.post(run_url, json=run_input, timeout=CONFIG["TIMEOUT"])

        if resp.status_code == 201:
            run_id = resp.json().get("data", {}).get("id")

            if run_id:
                for i in range(45):
                    time.sleep(2)
                    if status_callback:
                        status_callback(f"ğŸ“Š êµ¬ê¸€ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì¤‘... ({i*2}/90ì´ˆ)")

                    status_url = f"https://api.apify.com/v2/actor-runs/{run_id}?token={api_key}"
                    status_resp = requests.get(status_url, timeout=10)

                    if status_resp.status_code == 200:
                        status = status_resp.json().get("data", {}).get("status")

                        if status == "SUCCEEDED":
                            dataset_id = status_resp.json().get("data", {}).get("defaultDatasetId")
                            if dataset_id:
                                items_url = f"https://api.apify.com/v2/datasets/{dataset_id}/items?token={api_key}"
                                items_resp = requests.get(items_url, timeout=10)

                                if items_resp.status_code == 200:
                                    return _parse_google_trends(items_resp.json(), candidates)
                            break
                        elif status in ["FAILED", "ABORTED", "TIMED-OUT"]:
                            break

    except Exception as e:
        logger.error(f"êµ¬ê¸€ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    return pd.DataFrame()


def _parse_google_trends(items: List[Dict], candidates: List[str]) -> pd.DataFrame:
    """êµ¬ê¸€ íŠ¸ë Œë“œ ë°ì´í„° íŒŒì‹±"""
    all_data = {}

    for item in items:
        timeline = item.get("interestOverTime", {}).get("timelineData", [])

        if timeline:
            for idx, name in enumerate(candidates):
                dates, values = [], []

                for point in timeline:
                    try:
                        ts = point.get("time")
                        if not ts:
                            continue

                        dt = datetime.fromtimestamp(int(ts))
                        val_list = point.get("value", [])
                        val = val_list[idx] if idx < len(val_list) else None

                        if val is not None:
                            dates.append(dt)
                            values.append(val)
                    except Exception:
                        continue

                if dates and values:
                    all_data[name] = pd.Series(values, index=dates)

    if all_data:
        df = pd.DataFrame(all_data)
        df.index = pd.to_datetime(df.index)
        return df.sort_index()

    return pd.DataFrame()


def get_news_trend(candidates: List[str]) -> pd.DataFrame:
    """ë‰´ìŠ¤ ì–¸ê¸‰ëŸ‰ ê¸°ë°˜ íŠ¸ë Œë“œ"""
    ddgs = DDGS()
    end_date = datetime.now()
    start_date = end_date - timedelta(days=CONFIG["TREND_DAYS"])
    all_counts = {}

    for name in candidates:
        try:
            time.sleep(random.uniform(0.3, 0.7))
            news = ddgs.news(f'"{name}"', region="kr-kr", safesearch="off", max_results=100)

            if news:
                date_counts = {}
                for article in news:
                    date_str = article.get('date', '')
                    try:
                        if date_str:
                            dt = pd.to_datetime(date_str).date()
                            if start_date.date() <= dt <= end_date.date():
                                week_start = dt - timedelta(days=dt.weekday())
                                date_counts[week_start] = date_counts.get(week_start, 0) + 1
                    except Exception:
                        continue

                if date_counts:
                    all_counts[name] = date_counts

        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨ ({name}): {e}")

    if not all_counts:
        return pd.DataFrame()

    # ì •ê·œí™”
    global_max = max([max(c.values()) for c in all_counts.values() if c], default=1)
    trend_data = {
        name: {k: (v / global_max) * 100 for k, v in counts.items()}
        for name, counts in all_counts.items()
    }

    # ëª¨ë“  ë‚ ì§œ ìˆ˜ì§‘
    all_dates = set()
    for data in trend_data.values():
        all_dates.update(data.keys())

    if all_dates:
        df_data = {name: [data.get(d, 0) for d in sorted(all_dates)]
                   for name, data in trend_data.items()}
        df = pd.DataFrame(df_data, index=sorted(all_dates))
        df.index = pd.to_datetime(df.index)
        return df

    return pd.DataFrame()


def get_all_trends(candidates: List[str], keys: Dict,
                   status_container) -> Dict[str, pd.DataFrame]:
    """ëª¨ë“  íŠ¸ë Œë“œ ë°ì´í„° í†µí•© ìˆ˜ì§‘"""
    results = {"naver": pd.DataFrame(), "google": pd.DataFrame()}

    # ë„¤ì´ë²„ íŠ¸ë Œë“œ
    status_container.info("ğŸ“Š ë„¤ì´ë²„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì¤‘...")
    results["naver"] = get_naver_trend(
        tuple(candidates), keys["naver_id"], keys["naver_secret"]
    )

    # êµ¬ê¸€ íŠ¸ë Œë“œ (Apify)
    if keys.get("apify"):
        results["google"] = get_google_trend_apify(
            candidates, keys["apify"],
            lambda msg: status_container.info(msg)
        )

    # Apify ì‹¤íŒ¨ ì‹œ ë‰´ìŠ¤ ì–¸ê¸‰ëŸ‰
    if results["google"].empty:
        status_container.info("ğŸ“Š ë‰´ìŠ¤ ì–¸ê¸‰ëŸ‰ ìˆ˜ì§‘ ì¤‘...")
        results["google"] = get_news_trend(candidates)

    return results


# ==========================================
# í›„ë³´ì ë°ì´í„° ìˆ˜ì§‘
# ==========================================
def collect_candidate_data(keyword: str, election_name: str) -> Dict[str, Dict]:
    """í›„ë³´ì ê´€ë ¨ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬)"""
    ddgs = DDGS()
    collected = {
        "news": {"text": [], "links": []},
        "sns": {"text": [], "links": []},
        "community": {"text": [], "links": []},
        "wiki": {"text": [], "links": []},
        "youtube": {"text": [], "links": []}
    }

    def _collect_news():
        try:
            time.sleep(random.uniform(0.3, 0.6))
            news = ddgs.news(f'"{keyword}" 2025 OR 2026', region="kr-kr",
                           safesearch="off", max_results=15)
            if not news or len(news) < 3:
                news = ddgs.news(f'"{keyword}" {election_name}', region="kr-kr",
                               safesearch="off", max_results=15)

            for r in (news or []):
                title, body = r.get('title', ''), r.get('body', '')
                if contains_name(title, keyword) or contains_name(body, keyword):
                    date_str = r.get('date', '')
                    collected["news"]["text"].append(f"[{date_str}] {title}: {body[:200]}")
                    collected["news"]["links"].append({
                        "title": title[:60],
                        "url": r.get('url', '#'),
                        "source": r.get('source', ''),
                        "date": date_str,
                        "body": body[:300]
                    })
        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    def _collect_wiki():
        try:
            time.sleep(random.uniform(0.3, 0.6))
            wiki = ddgs.text(f'"{keyword}" (site:namu.wiki OR site:ko.wikipedia.org)',
                           region="kr-kr", safesearch="off", max_results=5)
            for r in (wiki or []):
                title, body = r.get('title', ''), r.get('body', '')
                if contains_name(title, keyword) or contains_name(body, keyword):
                    collected["wiki"]["text"].append(f"{title}: {body[:300]}")
                    collected["wiki"]["links"].append({
                        "title": title[:60],
                        "url": r.get('href', '#')
                    })
        except Exception as e:
            logger.warning(f"ìœ„í‚¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    def _collect_profile():
        try:
            time.sleep(random.uniform(0.3, 0.6))
            profile = ddgs.text(f'"{keyword}" ì •ë‹¹ ì†Œì† í˜„ì¬ 2025',
                              region="kr-kr", safesearch="off", max_results=5)
            for r in (profile or []):
                title, body = r.get('title', ''), r.get('body', '')
                if contains_name(title, keyword) or contains_name(body, keyword):
                    collected["wiki"]["text"].insert(0, f"{title}: {body[:250]}")
                    collected["wiki"]["links"].insert(0, {
                        "title": title[:60],
                        "url": r.get('href', '#')
                    })
        except Exception as e:
            logger.warning(f"í”„ë¡œí•„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    def _collect_sns():
        try:
            time.sleep(random.uniform(0.3, 0.6))
            sns = ddgs.text(
                f'"{keyword}" (site:blog.naver.com OR site:cafe.naver.com OR site:tistory.com)',
                region="kr-kr", safesearch="off", max_results=15
            )
            for r in (sns or []):
                title, body = r.get('title', ''), r.get('body', '')
                if contains_name(title, keyword) or contains_name(body, keyword):
                    url = r.get('href', '')
                    source = ("ë„¤ì´ë²„ë¸”ë¡œê·¸" if 'blog.naver' in url else
                             "ë„¤ì´ë²„ì¹´í˜" if 'cafe.naver' in url else
                             "í‹°ìŠ¤í† ë¦¬" if 'tistory' in url else "SNS")
                    collected["sns"]["text"].append(f"[{source}] {title}: {body[:150]}")
                    collected["sns"]["links"].append({
                        "title": title[:60],
                        "url": url,
                        "source": source
                    })
        except Exception as e:
            logger.warning(f"SNS ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    def _collect_community():
        try:
            time.sleep(random.uniform(0.3, 0.6))
            comm = ddgs.text(f'"{keyword}" (site:dcinside.com OR site:clien.net OR site:fmkorea.com)',
                           region="kr-kr", safesearch="off", max_results=10)
            for r in (comm or []):
                title, body = r.get('title', ''), r.get('body', '')
                if contains_name(title, keyword) or contains_name(body, keyword):
                    url = r.get('href', '')
                    source = ("ë””ì‹œì¸ì‚¬ì´ë“œ" if 'dcinside' in url else
                             "í´ë¦¬ì•™" if 'clien' in url else
                             "FMì½”ë¦¬ì•„" if 'fmkorea' in url else "ì»¤ë®¤ë‹ˆí‹°")
                    collected["community"]["text"].append(f"[{source}] {title}: {body[:150]}")
                    collected["community"]["links"].append({
                        "title": title[:60],
                        "url": url,
                        "source": source
                    })
        except Exception as e:
            logger.warning(f"ì»¤ë®¤ë‹ˆí‹° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    def _collect_youtube():
        try:
            time.sleep(random.uniform(0.3, 0.6))
            videos = ddgs.videos(f'"{keyword}" {election_name}',
                               region="kr-kr", safesearch="off", max_results=5)
            for r in (videos or []):
                title = r.get('title', '')
                if contains_name(title, keyword):
                    collected["youtube"]["text"].append(title)
                    collected["youtube"]["links"].append({
                        "title": title[:60],
                        "url": r.get('content', '#')
                    })
        except Exception as e:
            logger.warning(f"ìœ íŠœë¸Œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # ë³‘ë ¬ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=CONFIG["PARALLEL_WORKERS"]) as executor:
        futures = [
            executor.submit(_collect_news),
            executor.submit(_collect_wiki),
            executor.submit(_collect_profile),
            executor.submit(_collect_sns),
            executor.submit(_collect_community),
            executor.submit(_collect_youtube)
        ]
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                logger.warning(f"ë°ì´í„° ìˆ˜ì§‘ ì‘ì—… ì‹¤íŒ¨: {e}")

    return collected


# ==========================================
# AI ë¶„ì„
# ==========================================
def detect_party(text: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ ì •ë‹¹ ê°ì§€"""
    party_patterns = {
        "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹": ["ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹", "ë¯¼ì£¼ë‹¹ ì†Œì†", "ë¯¼ì£¼ë‹¹ ê³µì²œ", "ë¯¼ì£¼ë‹¹ì›"],
        "êµ­ë¯¼ì˜í˜": ["êµ­ë¯¼ì˜í˜", "êµ­ë¯¼ì˜í˜ ì†Œì†", "êµ­ë¯¼ì˜í˜ ê³µì²œ"],
        "ê°œí˜ì‹ ë‹¹": ["ê°œí˜ì‹ ë‹¹"],
        "ì •ì˜ë‹¹": ["ì •ì˜ë‹¹"],
        "ë¬´ì†Œì†": ["ë¬´ì†Œì†"]
    }

    max_matches = 0
    detected = "ì •ë³´ ë¶€ì¡±"

    for party, patterns in party_patterns.items():
        matches = sum(text.count(p) for p in patterns)
        if matches > max_matches:
            max_matches = matches
            detected = party

    return detected


def extract_career_timeline(news_links: List[Dict]) -> List[str]:
    """ë‰´ìŠ¤ì—ì„œ ê²½ë ¥ íƒ€ì„ë¼ì¸ ì¶”ì¶œ"""
    timeline = []
    governments = [
        (pd.Timestamp('2013-02-25'), "ì´ëª…ë°• ì •ë¶€"),
        (pd.Timestamp('2017-05-10'), "ë°•ê·¼í˜œ ì •ë¶€"),
        (pd.Timestamp('2022-05-10'), "ë¬¸ì¬ì¸ ì •ë¶€"),
        (pd.Timestamp('2025-05-10'), "ìœ¤ì„ì—´ ì •ë¶€"),
    ]

    for link in news_links:
        date_str = link.get('date', '')
        if not date_str:
            continue

        try:
            article_date = pd.to_datetime(date_str)
            content = f"{link.get('title', '')} {link.get('body', '')}"

            # ì •ë¶€ ì‹œê¸° íŒë³„
            gov = "í˜„ ì •ë¶€"
            for cutoff, name in governments:
                if article_date < cutoff:
                    gov = name
                    break

            # ê²½ë ¥ ì¶”ì¶œ
            careers = []
            if "ì²­ì™€ëŒ€" in content or "ëŒ€í†µë ¹ì‹¤" in content:
                if "ìˆ˜ì„" in content:
                    careers.append(f"{gov} ìˆ˜ì„")
                else:
                    careers.append(f"{gov} ì²­ì™€ëŒ€" if "ì´ëª…ë°•" in gov or "ë°•ê·¼í˜œ" in gov or "ë¬¸ì¬ì¸" in gov else f"{gov} ëŒ€í†µë ¹ì‹¤")

            if "êµ­íšŒì˜ì›" in content:
                for term in ["22ëŒ€", "21ëŒ€", "20ëŒ€", "19ëŒ€", "18ëŒ€"]:
                    if term in content:
                        careers.append(f"{term} êµ­íšŒì˜ì›")
                        break

            if "ë„ì§€ì‚¬" in content and "í›„ë³´" not in content:
                careers.append("ê´‘ì—­ë‹¨ì²´ì¥")
            if "ì¥ê´€" in content:
                careers.append("ì¥ê´€")

            timeline.extend(careers)

        except Exception:
            continue

    # ì¤‘ë³µ ì œê±°
    seen = set()
    unique = []
    for item in timeline:
        if item not in seen:
            seen.add(item)
            unique.append(item)

    return unique[:5]


def analyze_candidate(model, name: str, collected: Dict,
                      trend_info: str) -> Dict:
    """í›„ë³´ì AI ë¶„ì„"""
    news_cnt = len(collected.get("news", {}).get("links", []))
    sns_cnt = len(collected.get("sns", {}).get("links", []))
    community_cnt = len(collected.get("community", {}).get("links", []))

    # ê²½ë ¥ ì¶”ì¶œ
    career_timeline = extract_career_timeline(
        collected.get("news", {}).get("links", [])
    )

    # í…ìŠ¤íŠ¸ ë³‘í•©
    all_text = (
        collected.get("news", {}).get("text", [])[:5] +
        collected.get("wiki", {}).get("text", [])[:5] +
        collected.get("sns", {}).get("text", [])[:3] +
        collected.get("community", {}).get("text", [])[:2]
    )
    raw_text = "\n".join(all_text)

    # ì •ë‹¹ ê°ì§€
    detected_party = detect_party(raw_text)

    # í™”ì œì„± ì ìˆ˜ ê³„ì‚°
    poll_est = 50
    try:
        if trend_info != "ë°ì´í„° ì—†ìŒ":
            match = re.search(r'í˜„ì¬\s*([\d.]+)', trend_info)
            if match:
                poll_est = min(int(float(match.group(1)) * 8), 100)
    except Exception:
        pass

    # ë°ì´í„° ë¶€ì¡± ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
    if not raw_text or len(raw_text) < 30:
        return {
            "name": name,
            "party": detected_party,
            "current_role": "ì •ë³´ ë¶€ì¡±",
            "past_career": ", ".join(career_timeline) if career_timeline else "ì •ë³´ ë¶€ì¡±",
            "poll_est": poll_est,
            "analysis": "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.",
            "sns_sentiment": "ë¶„ì„ ë¶ˆê°€",
            "keywords": [name]
        }

    career_info = ", ".join(career_timeline) if career_timeline else "ê²½ë ¥ ì •ë³´ ì—†ìŒ"

    prompt = f'''ë‹¤ìŒì€ {name} í›„ë³´ ì •ë³´ì…ë‹ˆë‹¤.

íŠ¸ë Œë“œ: {trend_info}
ì¶”ì • ê²½ë ¥: {career_info}

ìˆ˜ì§‘ëœ ë°ì´í„°:
{raw_text[:2500]}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”:
{{"name":"{name}","party":"ì •ë‹¹ëª…(ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹/êµ­ë¯¼ì˜í˜/ê°œí˜ì‹ ë‹¹/ë¬´ì†Œì† ì¤‘ í•˜ë‚˜)","current_role":"í˜„ì¬ ì§í•¨","past_career":"ì£¼ìš” ê²½ë ¥ ìš”ì•½","poll_est":{poll_est},"analysis":"ë‰´ìŠ¤ ê¸°ë°˜ ë¶„ì„ ìš”ì•½","sns_sentiment":"SNS ì—¬ë¡  ë¶„ì„","keywords":["ê´€ë ¨ í‚¤ì›Œë“œ 3-5ê°œ"]}}'''

    for attempt in range(CONFIG["MAX_RETRIES"]):
        try:
            resp = model.generate_content(prompt)
            result = clean_json(resp.text)

            if result and 'name' in result:
                # ìˆ˜ì¹˜ ì •ê·œí™”
                if isinstance(result.get('poll_est'), str):
                    nums = re.findall(r'\d+', str(result['poll_est']))
                    result['poll_est'] = int(nums[0]) if nums else poll_est

                # ì •ë‹¹ ë³´ì™„
                if not result.get('party') or result['party'] in ['ì •ë³´ ì—†ìŒ', '']:
                    result['party'] = detected_party

                # ê²½ë ¥ ë³´ì™„
                if not result.get('past_career') or result['past_career'] == 'ì •ë³´ ì—†ìŒ':
                    result['past_career'] = ", ".join(career_timeline) if career_timeline else "ì •ë³´ ë¶€ì¡±"

                return result

        except Exception as e:
            logger.warning(f"AI ë¶„ì„ ì‹¤íŒ¨ ({attempt + 1}/{CONFIG['MAX_RETRIES']}): {e}")
            time.sleep(CONFIG["RETRY_DELAY"] * (attempt + 1))

    # í´ë°± ê²°ê³¼
    return {
        "name": name,
        "party": detected_party,
        "current_role": "ì •ë³´ í™•ì¸ í•„ìš”",
        "past_career": ", ".join(career_timeline) if career_timeline else "ì •ë³´ ë¶€ì¡±",
        "poll_est": poll_est,
        "analysis": f"{name} í›„ë³´: ë‰´ìŠ¤ {news_cnt}ê±´, SNS {sns_cnt}ê±´, ì»¤ë®¤ë‹ˆí‹° {community_cnt}ê±´ ë¶„ì„",
        "sns_sentiment": f"SNS/ì»¤ë®¤ë‹ˆí‹° {sns_cnt + community_cnt}ê±´ ìˆ˜ì§‘",
        "keywords": [name, detected_party] if detected_party != "ì •ë³´ ë¶€ì¡±" else [name]
    }


# ==========================================
# ì˜ˆì¸¡
# ==========================================
def predict_future(df: pd.DataFrame, days: int = 30) -> pd.DataFrame:
    """ë¯¸ë˜ íŠ¸ë Œë“œ ì˜ˆì¸¡ (ì„ í˜• íšŒê·€)"""
    if df.empty:
        return pd.DataFrame()

    future_df = pd.DataFrame()
    last_date = df.index[-1]
    future_dates = [last_date + timedelta(days=i) for i in range(1, days + 1)]

    for col in df.columns:
        series = df[col].dropna()
        if len(series) < 5:
            continue

        recent = series.tail(min(30, len(series)))
        x = np.arange(len(recent))
        y = recent.values

        if len(x) > 1:
            try:
                z = np.polyfit(x, y, 1)
                p = np.poly1d(z)
                predictions = p(np.arange(len(recent), len(recent) + days))
                min_val = max(recent.iloc[-1] * 0.3, 1)
                future_df[col] = np.clip(predictions, min_val, 100)
            except Exception as e:
                logger.warning(f"ì˜ˆì¸¡ ì‹¤íŒ¨ ({col}): {e}")

    if not future_df.empty:
        future_df.index = future_dates

    return future_df


# ==========================================
# ì‹œê°í™”
# ==========================================
def create_trend_chart(trends: Dict[str, pd.DataFrame],
                       pred_trends: Dict[str, pd.DataFrame],
                       candidates: List[str],
                       google_source: str) -> go.Figure:
    """íŠ¸ë Œë“œ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
    colors = CONFIG["COLORS"]
    naver_df = trends.get("naver", pd.DataFrame())
    google_df = trends.get("google", pd.DataFrame())
    pred_naver = pred_trends.get("naver", pd.DataFrame())
    pred_google = pred_trends.get("google", pd.DataFrame())

    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=("ğŸŸ¢ ë„¤ì´ë²„ íŠ¸ë Œë“œ", f"ğŸ”µ {google_source}"),
        horizontal_spacing=0.08
    )

    # ë„¤ì´ë²„ ì°¨íŠ¸
    if not naver_df.empty:
        for idx, col in enumerate(candidates):
            if col in naver_df.columns:
                color = colors[idx % len(colors)]
                fig.add_trace(
                    go.Scatter(
                        x=naver_df.index, y=naver_df[col],
                        mode='lines', name=col,
                        line=dict(color=color, width=2.5),
                        legendgroup=col
                    ),
                    row=1, col=1
                )
                if not pred_naver.empty and col in pred_naver.columns:
                    pred_x = [naver_df.index[-1]] + list(pred_naver.index)
                    pred_y = [naver_df[col].iloc[-1]] + list(pred_naver[col])
                    fig.add_trace(
                        go.Scatter(
                            x=pred_x, y=pred_y, mode='lines',
                            line=dict(color=color, width=2.5, dash='dot'),
                            legendgroup=col, showlegend=False,
                            hovertemplate='ì˜ˆì¸¡: %{y:.1f}<extra></extra>'
                        ),
                        row=1, col=1
                    )

    # êµ¬ê¸€/ë‰´ìŠ¤ ì°¨íŠ¸
    if not google_df.empty:
        for idx, col in enumerate(candidates):
            if col in google_df.columns:
                color = colors[idx % len(colors)]
                fig.add_trace(
                    go.Scatter(
                        x=google_df.index, y=google_df[col],
                        mode='lines',
                        line=dict(color=color, width=2.5),
                        legendgroup=col, showlegend=False
                    ),
                    row=1, col=2
                )
                if not pred_google.empty and col in pred_google.columns:
                    pred_x = [google_df.index[-1]] + list(pred_google.index)
                    pred_y = [google_df[col].iloc[-1]] + list(pred_google[col])
                    fig.add_trace(
                        go.Scatter(
                            x=pred_x, y=pred_y, mode='lines',
                            line=dict(color=color, width=2.5, dash='dot'),
                            legendgroup=col, showlegend=False
                        ),
                        row=1, col=2
                    )
    else:
        fig.add_annotation(
            text="ë°ì´í„° ì—†ìŒ", xref="x2", yref="y2",
            x=0.5, y=0.5, showarrow=False,
            font=dict(size=16, color="#94a3b8")
        )

    fig.update_layout(
        height=420,
        template="plotly_dark",
        paper_bgcolor='#0f172a',
        plot_bgcolor='#1e293b',
        font=dict(color='#e2e8f0', size=12),
        legend=dict(
            orientation="h",
            yanchor="bottom", y=-0.25,
            xanchor="center", x=0.5,
            bgcolor='rgba(0,0,0,0)'
        ),
        margin=dict(l=50, r=50, t=60, b=100),
        hovermode='x unified'
    )

    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#334155', tickangle=-45)
    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='#334155', title_text="ê²€ìƒ‰ ì§€ìˆ˜")

    return fig


def create_radar_chart(results: List[Dict], trends: Dict) -> go.Figure:
    """í›„ë³´ì ë¹„êµ ë ˆì´ë” ì°¨íŠ¸"""
    categories = ['í™”ì œì„±', 'ë‰´ìŠ¤ ë…¸ì¶œ', 'SNS ë°˜ì‘', 'ê²€ìƒ‰ íŠ¸ë Œë“œ', 'ì¢…í•© ì ìˆ˜']

    fig = go.Figure()
    colors = CONFIG["COLORS"]

    naver_df = trends.get("naver", pd.DataFrame())

    for idx, candidate in enumerate(results):
        name = candidate.get('name', '')

        # ì ìˆ˜ ê³„ì‚°
        poll_est = candidate.get('poll_est', 50)

        naver_score = 50
        if not naver_df.empty and name in naver_df.columns:
            naver_score = min(float(naver_df[name].mean()), 100)

        # ê°€ìƒ ì ìˆ˜ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ê³„ì‚° í•„ìš”)
        news_score = min(poll_est * 1.1, 100)
        sns_score = min(poll_est * 0.9, 100)
        total_score = (poll_est + naver_score + news_score + sns_score) / 4

        values = [poll_est, news_score, sns_score, naver_score, total_score]
        values.append(values[0])  # ë‹«íŒ í˜•íƒœ

        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=categories + [categories[0]],
            fill='toself',
            name=name,
            line=dict(color=colors[idx % len(colors)], width=2),
            fillcolor=f"rgba{tuple(list(int(colors[idx % len(colors)].lstrip('#')[i:i+2], 16) for i in (0, 2, 4)) + [0.2])}"
        ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100],
                gridcolor='#334155'
            ),
            angularaxis=dict(gridcolor='#334155'),
            bgcolor='#1e293b'
        ),
        showlegend=True,
        legend=dict(
            orientation="h",
            yanchor="bottom", y=-0.2,
            xanchor="center", x=0.5
        ),
        paper_bgcolor='#0f172a',
        font=dict(color='#e2e8f0'),
        height=400,
        margin=dict(l=80, r=80, t=40, b=80)
    )

    return fig


def create_comparison_bar(results: List[Dict], trends: Dict) -> go.Figure:
    """í›„ë³´ì ë¹„êµ ë§‰ëŒ€ ì°¨íŠ¸"""
    names = [r['name'] for r in results]
    poll_scores = [r.get('poll_est', 0) for r in results]

    naver_df = trends.get("naver", pd.DataFrame())
    naver_scores = []
    for name in names:
        if not naver_df.empty and name in naver_df.columns:
            naver_scores.append(float(naver_df[name].iloc[-1]))
        else:
            naver_scores.append(0)

    fig = go.Figure()

    fig.add_trace(go.Bar(
        name='í™”ì œì„± ì ìˆ˜',
        x=names,
        y=poll_scores,
        marker_color='#3b82f6'
    ))

    fig.add_trace(go.Bar(
        name='ë„¤ì´ë²„ íŠ¸ë Œë“œ',
        x=names,
        y=naver_scores,
        marker_color='#22c55e'
    ))

    fig.update_layout(
        barmode='group',
        template="plotly_dark",
        paper_bgcolor='#0f172a',
        plot_bgcolor='#1e293b',
        font=dict(color='#e2e8f0'),
        height=350,
        legend=dict(orientation="h", y=1.1, x=0.5, xanchor="center"),
        margin=dict(l=50, r=50, t=60, b=50)
    )

    fig.update_xaxes(showgrid=False)
    fig.update_yaxes(showgrid=True, gridcolor='#334155')

    return fig


# ==========================================
# UI ì»´í¬ë„ŒíŠ¸
# ==========================================
def render_candidate_card(candidate: Dict, collected: Dict,
                          trends: Dict, pred_trends: Dict,
                          google_label: str):
    """í›„ë³´ì ì¹´ë“œ ë Œë”ë§"""
    name = candidate.get('name', 'ë¯¸ìƒ')
    party = candidate.get('party', 'ì •ë³´ ì—†ìŒ')
    party_color = get_party_color(party)

    naver_df = trends.get("naver", pd.DataFrame())
    google_df = trends.get("google", pd.DataFrame())
    pred_naver = pred_trends.get("naver", pd.DataFrame())
    pred_google = pred_trends.get("google", pd.DataFrame())

    # íŠ¸ë Œë“œ ë°°ì§€
    curr_val = safe_get_value(naver_df, name)
    fut_val = safe_get_value(pred_naver, name)

    badge = ""
    badge_color = "#6b7280"
    if curr_val is not None and fut_val is not None:
        if fut_val > curr_val * 1.1:
            badge = "ğŸ“ˆ ìƒìŠ¹ì˜ˆì¸¡"
            badge_color = "#22c55e"
        elif fut_val < curr_val * 0.9:
            badge = "ğŸ“‰ í•˜ë½ì˜ˆì¸¡"
            badge_color = "#ef4444"
        else:
            badge = "â¡ï¸ ìœ ì§€"
            badge_color = "#f59e0b"

    # ì¹´ë“œ í—¤ë”
    with st.container():
        col1, col2 = st.columns([4, 1])
        with col1:
            badge_html = f"<span style='background:{badge_color};color:white;padding:4px 10px;border-radius:15px;font-size:0.7em;margin-left:10px;'>{badge}</span>" if badge else ""
            st.markdown(
                f"### {name} <span style='background:{party_color};color:white;padding:5px 14px;border-radius:8px;font-size:0.5em;margin-left:12px;'>{party}</span>{badge_html}",
                unsafe_allow_html=True
            )
        with col2:
            st.metric("ğŸ”¥ í™”ì œì„±", f"{candidate.get('poll_est', 0)}")

        # íŠ¸ë Œë“œ ë©”íŠ¸ë¦­
        m1, m2, m3, m4 = st.columns(4)
        with m1:
            val = safe_get_value(naver_df, name)
            st.metric("ë„¤ì´ë²„ í˜„ì¬", f"{val:.1f}" if val else "-")
        with m2:
            val = safe_get_value(pred_naver, name)
            st.metric("ë„¤ì´ë²„ ì˜ˆì¸¡", f"{val:.1f}" if val else "-")
        with m3:
            val = safe_get_value(google_df, name)
            st.metric(f"{google_label} í˜„ì¬", f"{val:.1f}" if val else "-")
        with m4:
            val = safe_get_value(pred_google, name)
            st.metric(f"{google_label} ì˜ˆì¸¡", f"{val:.1f}" if val else "-")

        # í”„ë¡œí•„ ì •ë³´
        c1, c2 = st.columns(2)
        with c1:
            st.markdown("**ğŸ”· í˜„ì¬ ì§í•¨**")
            st.info(candidate.get('current_role', 'ì •ë³´ ì—†ìŒ'))
        with c2:
            st.markdown("**âšª ì£¼ìš” ê²½ë ¥**")
            st.info(candidate.get('past_career', 'ì •ë³´ ì—†ìŒ'))

        # ë¶„ì„ ê²°ê³¼
        st.markdown("**ğŸ“° ë‰´ìŠ¤ ë¶„ì„**")
        st.write(candidate.get('analysis', ''))

        # SNS ì—¬ë¡ 
        st.markdown("**ğŸ’¬ SNS/ì»¤ë®¤ë‹ˆí‹° ì—¬ë¡ **")
        sns_sent = str(candidate.get('sns_sentiment', ''))
        if "ê¸ì •" in sns_sent:
            st.success(f"ğŸŸ¢ {sns_sent}")
        elif "ë¶€ì •" in sns_sent:
            st.error(f"ğŸ”´ {sns_sent}")
        else:
            st.warning(f"ğŸŸ¡ {sns_sent}")

        # í‚¤ì›Œë“œ
        keywords = candidate.get('keywords', [])
        if keywords:
            st.markdown(" ".join([f"`#{k}`" for k in keywords[:6]]))

        # ë°ì´í„° ì¶œì²˜
        st.markdown("**ğŸ“š ë°ì´í„° ì¶œì²˜**")
        tabs = st.tabs([
            f"ğŸ“° ë‰´ìŠ¤ ({len(collected.get('news', {}).get('links', []))})",
            f"ğŸ’¬ SNS ({len(collected.get('sns', {}).get('links', []))})",
            f"ğŸ‘¥ ì»¤ë®¤ë‹ˆí‹° ({len(collected.get('community', {}).get('links', []))})",
            f"ğŸ“– ìœ„í‚¤ ({len(collected.get('wiki', {}).get('links', []))})",
            f"ğŸ“º ìœ íŠœë¸Œ ({len(collected.get('youtube', {}).get('links', []))})"
        ])

        sources = ['news', 'sns', 'community', 'wiki', 'youtube']
        for tab, src in zip(tabs, sources):
            with tab:
                links = collected.get(src, {}).get('links', [])
                if links:
                    for link in links[:10]:
                        st.markdown(f"- [{link.get('title', 'ì œëª© ì—†ìŒ')}]({link.get('url', '#')})")
                else:
                    st.caption("ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")

        st.divider()


def render_history_section(db: DatabaseManager):
    """ë¶„ì„ íˆìŠ¤í† ë¦¬ ì„¹ì…˜"""
    history = db.get_history(limit=5)

    if history:
        st.markdown("**ğŸ“œ ìµœê·¼ ë¶„ì„ ê¸°ë¡**")
        for item in history:
            candidates = json.loads(item['candidates'])
            with st.expander(f"ğŸ“… {item['analysis_date'][:16]} - {item['election_name']}"):
                st.write(f"í›„ë³´ì: {', '.join(candidates)}")
                if st.button("ìƒì„¸ ë³´ê¸°", key=f"hist_{item['id']}"):
                    detail = db.get_analysis_detail(item['id'])
                    if detail:
                        st.json(detail)
    else:
        st.caption("ë¶„ì„ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")


def export_results(results: List[Dict], trends: Dict,
                   election: str) -> str:
    """ë¶„ì„ ê²°ê³¼ JSON ë‚´ë³´ë‚´ê¸°"""
    export_data = {
        "election": election,
        "analysis_date": datetime.now().isoformat(),
        "candidates": results,
        "trends_summary": {}
    }

    for source, df in trends.items():
        if not df.empty:
            export_data["trends_summary"][source] = {
                col: {
                    "current": safe_get_value(df, col),
                    "mean": float(df[col].mean()) if col in df.columns else None,
                    "max": float(df[col].max()) if col in df.columns else None
                }
                for col in df.columns
            }

    return json.dumps(export_data, ensure_ascii=False, indent=2)


# ==========================================
# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# ==========================================
def main():
    st.title("ğŸ—³ï¸ ì„ ê±° ì „ëµ ì¸ì‚¬ì´íŠ¸: ì˜ˆì¸¡ê³¼ ì „ë§")
    st.caption("ë„¤ì´ë²„ + êµ¬ê¸€(Apify) íŠ¸ë Œë“œ ì¢…í•© ë¶„ì„ ì‹œìŠ¤í…œ v2.0")

    # API í‚¤ ë¡œë“œ
    keys = load_api_keys()

    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    db = DatabaseManager()

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.markdown("### âš™ï¸ ì„¤ì •")

        # ì‹œìŠ¤í…œ ìƒíƒœ
        status_items = []
        status_items.append("âœ… Gemini API ì—°ê²°ë¨")
        status_items.append("âœ… ë„¤ì´ë²„ API ì—°ê²°ë¨")
        if keys.get("apify"):
            status_items.append("âœ… Apify ì—°ê²°ë¨")
        else:
            status_items.append("âš ï¸ Apify ë¯¸ì„¤ì • (ë‰´ìŠ¤ ì–¸ê¸‰ëŸ‰ ì‚¬ìš©)")

        for item in status_items:
            st.markdown(f"<small>{item}</small>", unsafe_allow_html=True)

        st.divider()

        # ì…ë ¥ ì„¤ì •
        election = st.text_input(
            "ğŸ“‹ ë¶„ì„ ëŒ€ìƒ ì„ ê±°",
            value="2026ë…„ ì¶©ì²­ë¶ë„ì§€ì‚¬ ì„ ê±°",
            help="ë¶„ì„í•  ì„ ê±°ëª…ì„ ì…ë ¥í•˜ì„¸ìš”"
        )

        st.markdown("**ğŸ‘¥ í›„ë³´ì ëª©ë¡** (ì¤„ë°”ê¿ˆ êµ¬ë¶„)")
        candidates_text = st.text_area(
            "",
            value="ì‹ ìš©í•œ\në…¸ì˜ë¯¼\nì†¡ê¸°ì„­",
            height=150,
            label_visibility="collapsed",
            help="ê° ì¤„ì— í›„ë³´ì ì´ë¦„ í•˜ë‚˜ì”© ì…ë ¥"
        )

        candidates = [c.strip() for c in candidates_text.split('\n') if c.strip()]

        # ìœ íš¨ì„± ê²€ì‚¬
        is_valid, error_msg = validate_candidates(candidates)
        if not is_valid:
            st.error(error_msg)
        else:
            st.success(f"âœ“ ë“±ë¡: {len(candidates)}ëª…")

        st.divider()

        # ë¶„ì„ ë²„íŠ¼
        start_analysis = st.button(
            "ğŸš€ ì¢…í•© ë¶„ì„ ì‹¤í–‰",
            type="primary",
            use_container_width=True,
            disabled=not is_valid
        )

        st.divider()

        # íˆìŠ¤í† ë¦¬
        with st.expander("ğŸ“œ ë¶„ì„ íˆìŠ¤í† ë¦¬"):
            render_history_section(db)

    # ë©”ì¸ ì»¨í…ì¸ 
    if start_analysis:
        model = genai.GenerativeModel(get_best_model(keys["gemini"]))

        status_container = st.empty()
        progress_bar = st.progress(0)

        try:
            # 1. íŠ¸ë Œë“œ ìˆ˜ì§‘
            status_container.info("ğŸ“Š íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            trends = get_all_trends(candidates, keys, status_container)
            progress_bar.progress(0.2)

            google_source = "êµ¬ê¸€ íŠ¸ë Œë“œ" if keys.get("apify") and not trends["google"].empty else "ë‰´ìŠ¤ ì–¸ê¸‰ëŸ‰"
            google_label = "êµ¬ê¸€" if "êµ¬ê¸€" in google_source else "ë‰´ìŠ¤"

            # 2. ì˜ˆì¸¡ ê³„ì‚°
            status_container.info("ğŸ”® ë¯¸ë˜ íŠ¸ë Œë“œ ì˜ˆì¸¡ ì¤‘...")
            pred_trends = {
                "naver": predict_future(trends["naver"], CONFIG["PREDICTION_DAYS"]),
                "google": predict_future(trends["google"], CONFIG["PREDICTION_DAYS"])
            }
            progress_bar.progress(0.3)

            # 3. í›„ë³´ìë³„ ë¶„ì„
            results = []
            all_collected = {}

            for i, name in enumerate(candidates):
                status_container.info(f"âš¡ [{i+1}/{len(candidates)}] {name} ë¶„ì„ ì¤‘...")

                # ë°ì´í„° ìˆ˜ì§‘
                collected = collect_candidate_data(name, election)
                all_collected[name] = collected

                # íŠ¸ë Œë“œ ì •ë³´ êµ¬ì„±
                curr = safe_get_value(trends.get("naver", pd.DataFrame()), name)
                fut = safe_get_value(pred_trends.get("naver", pd.DataFrame()), name)

                trend_info = "ë°ì´í„° ì—†ìŒ"
                if curr is not None:
                    fut = fut if fut is not None else curr
                    trend_info = f"í˜„ì¬ {curr:.1f}, ì˜ˆì¸¡ {fut:.1f}"

                # AI ë¶„ì„
                result = analyze_candidate(model, name, collected, trend_info)
                results.append(result)

                progress_bar.progress(0.3 + (0.6 * (i + 1) / len(candidates)))
                time.sleep(1.5)

            # 4. ê²°ê³¼ ì €ì¥
            status_container.info("ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
            analysis_id = db.save_analysis(election, candidates, results, trends)

            progress_bar.progress(1.0)
            status_container.empty()
            progress_bar.empty()

            st.success(f"âœ… ë¶„ì„ ì™„ë£Œ! (ID: {analysis_id})")

            # ê²°ê³¼ í‘œì‹œ
            # íƒ­ êµ¬ì„±
            tab1, tab2, tab3, tab4 = st.tabs([
                "ğŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„", "ğŸ¯ í›„ë³´ ë¹„êµ", "ğŸ“‹ ìƒì„¸ ë¦¬í¬íŠ¸", "ğŸ’¾ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"
            ])

            with tab1:
                st.subheader("ğŸ“ˆ íŠ¸ë Œë“œ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜")
                st.caption("ì‹¤ì„ : ì‹¤ì œ ë°ì´í„° | ì ì„ : 30ì¼ ì˜ˆì¸¡")
                st.plotly_chart(
                    create_trend_chart(trends, pred_trends, candidates, google_source),
                    use_container_width=True
                )

                # ìš”ì•½ í…Œì´ë¸”
                st.markdown("**ğŸ“Š íŠ¸ë Œë“œ ìˆ˜ì¹˜ ìš”ì•½**")
                summary_data = []
                for name in candidates:
                    row = {"í›„ë³´": name}
                    row["ë„¤ì´ë²„ í˜„ì¬"] = f"{safe_get_value(trends['naver'], name):.1f}" if safe_get_value(trends['naver'], name) else "-"
                    row["ë„¤ì´ë²„ ì˜ˆì¸¡"] = f"{safe_get_value(pred_trends['naver'], name):.1f}" if safe_get_value(pred_trends['naver'], name) else "-"
                    row[f"{google_label} í˜„ì¬"] = f"{safe_get_value(trends['google'], name):.1f}" if safe_get_value(trends['google'], name) else "-"
                    row[f"{google_label} ì˜ˆì¸¡"] = f"{safe_get_value(pred_trends['google'], name):.1f}" if safe_get_value(pred_trends['google'], name) else "-"
                    summary_data.append(row)

                st.dataframe(
                    pd.DataFrame(summary_data),
                    use_container_width=True,
                    hide_index=True
                )

            with tab2:
                st.subheader("ğŸ¯ í›„ë³´ì ë¹„êµ ë¶„ì„")

                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**ë ˆì´ë” ì°¨íŠ¸**")
                    st.plotly_chart(
                        create_radar_chart(results, trends),
                        use_container_width=True
                    )

                with col2:
                    st.markdown("**ì ìˆ˜ ë¹„êµ**")
                    st.plotly_chart(
                        create_comparison_bar(results, trends),
                        use_container_width=True
                    )

                # ìˆœìœ„í‘œ
                st.markdown("**ğŸ† í™”ì œì„± ìˆœìœ„**")
                sorted_results = sorted(results, key=lambda x: x.get('poll_est', 0), reverse=True)
                for i, r in enumerate(sorted_results):
                    medal = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else f"{i+1}."
                    st.markdown(f"{medal} **{r['name']}** ({r.get('party', 'ì •ë³´ ì—†ìŒ')}) - í™”ì œì„±: {r.get('poll_est', 0)}")

            with tab3:
                st.subheader("ğŸ“‹ í›„ë³´ìë³„ ì‹¬ì¸µ ë¦¬í¬íŠ¸")
                for result in results:
                    render_candidate_card(
                        result,
                        all_collected.get(result['name'], {}),
                        trends,
                        pred_trends,
                        google_label
                    )

            with tab4:
                st.subheader("ğŸ’¾ ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")

                export_json = export_results(results, trends, election)

                st.download_button(
                    label="ğŸ“¥ JSON ë‹¤ìš´ë¡œë“œ",
                    data=export_json,
                    file_name=f"election_analysis_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
                    mime="application/json"
                )

                with st.expander("JSON ë¯¸ë¦¬ë³´ê¸°"):
                    st.code(export_json, language="json")

        except Exception as e:
            status_container.empty()
            progress_bar.empty()
            st.error(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"ë¶„ì„ ì˜¤ë¥˜: {e}", exc_info=True)

    else:
        # ì‹œì‘ ì „ ì•ˆë‚´
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì„ ê±°ì™€ í›„ë³´ìë¥¼ ì„¤ì •í•œ í›„ 'ì¢…í•© ë¶„ì„ ì‹¤í–‰' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

        # ê¸°ëŠ¥ ì†Œê°œ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown("""
            ### ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„
            - ë„¤ì´ë²„ ë°ì´í„°ë©
            - êµ¬ê¸€ íŠ¸ë Œë“œ (Apify)
            - ë‰´ìŠ¤ ì–¸ê¸‰ëŸ‰
            - 30ì¼ ë¯¸ë˜ ì˜ˆì¸¡
            """)
        with col2:
            st.markdown("""
            ### ğŸ¤– AI ë¶„ì„
            - Gemini ê¸°ë°˜ ë¶„ì„
            - ì •ë‹¹/ê²½ë ¥ ìë™ ê°ì§€
            - SNS ì—¬ë¡  ë¶„ì„
            - í‚¤ì›Œë“œ ì¶”ì¶œ
            """)
        with col3:
            st.markdown("""
            ### ğŸ“ˆ ì‹œê°í™”
            - íŠ¸ë Œë“œ ë¹„êµ ì°¨íŠ¸
            - ë ˆì´ë” ì°¨íŠ¸
            - ìˆœìœ„ ë¹„êµ
            - ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
            """)


if __name__ == "__main__":
    main()
