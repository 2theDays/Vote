import streamlit as st
import google.generativeai as genai
from duckduckgo_search import DDGS
import requests
import time
import json
import random
import re
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì„ ê±° ì „ëµ: ì˜ˆì¸¡ê³¼ ì „ë§", layout="wide", page_icon="ğŸ“¡")

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
.main { background-color: #0f172a; color: #e2e8f0; }
h1, h2, h3 { color: #f1f5f9; }
.stButton>button { background-color: #3b82f6; color: white; border-radius: 8px; height: 50px; font-size: 16px; font-weight: 600; }
</style>
""", unsafe_allow_html=True)

# ì„¤ì • ìƒìˆ˜
CONFIG = {
    "MAX_CANDIDATES": 10,
    "TREND_DAYS": 180,
    "PREDICTION_DAYS": 30,
    "MAX_NEWS": 20,
    "TIMEOUT": 30,
    "COLORS": ['#3b82f6', '#ef4444', '#22c55e', '#f59e0b', '#8b5cf6', '#ec4899', '#06b6d4', '#84cc16']
}


def load_api_keys():
    """API í‚¤ ë¡œë“œ"""
    try:
        keys = {
            "gemini": st.secrets["GEMINI_API_KEY"],
            "naver_id": st.secrets["NAVER_CLIENT_ID"],
            "naver_secret": st.secrets["NAVER_CLIENT_SECRET"],
        }
        try:
            keys["apify"] = st.secrets["APIFY_API_KEY"]
        except:
            keys["apify"] = None
        return keys
    except Exception as e:
        logger.error(f"API í‚¤ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        st.error("API í‚¤ ë¡œë“œ ì‹¤íŒ¨. `.streamlit/secrets.toml` í™•ì¸")
        st.stop()


def validate_candidates(candidates):
    """í›„ë³´ì ì´ë¦„ ìœ íš¨ì„± ê²€ì‚¬"""
    if not candidates:
        return False, "í›„ë³´ìë¥¼ ìµœì†Œ 1ëª… ì…ë ¥í•˜ì„¸ìš”"
    if len(candidates) > CONFIG["MAX_CANDIDATES"]:
        return False, f"í›„ë³´ìëŠ” ìµœëŒ€ {CONFIG['MAX_CANDIDATES']}ëª…ê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤"
    for c in candidates:
        if len(c) < 2:
            return False, f"'{c}'ëŠ” ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 2ì)"
    return True, ""


def get_best_model(api_key):
    """ìµœì ì˜ Gemini ëª¨ë¸ ì„ íƒ"""
    genai.configure(api_key=api_key)
    try:
        models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        for m in models:
            if 'flash' in m.lower():
                return m
        if models:
            return models[0]
    except Exception as e:
        logger.warning(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
    return "models/gemini-1.5-flash"


def clean_json(text):
    """JSON í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return None
    try:
        text = re.sub(r'```json\s*|```\s*', '', text).strip()
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
        if match:
            return json.loads(match.group(0).replace('\n', ' '))
        return json.loads(text)
    except Exception as e:
        logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return None


def safe_get_last_value(df, col_name):
    """ì•ˆì „í•œ ë§ˆì§€ë§‰ ê°’ ì¶”ì¶œ"""
    if df.empty or col_name not in df.columns or len(df) == 0:
        return None
    try:
        return df[col_name].iloc[-1]
    except Exception as e:
        logger.warning(f"ê°’ ì¶”ì¶œ ì‹¤íŒ¨ ({col_name}): {str(e)}")
        return None


@st.cache_data(ttl=3600)
def get_naver_trend(candidates, n_id, n_secret):
    """ë„¤ì´ë²„ íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘"""
    try:
        url = "https://openapi.naver.com/v1/datalab/search"
        headers = {
            "X-Naver-Client-Id": n_id,
            "X-Naver-Client-Secret": n_secret,
            "Content-Type": "application/json"
        }
        end = datetime.now()
        start = end - timedelta(days=CONFIG["TREND_DAYS"])
        all_data = {}
        
        for i in range(0, len(candidates), 5):
            batch = candidates[i:i+5]
            body = {
                "startDate": start.strftime("%Y-%m-%d"),
                "endDate": end.strftime("%Y-%m-%d"),
                "timeUnit": "week",
                "keywordGroups": [{"groupName": c, "keywords": [c]} for c in batch]
            }
            resp = requests.post(url, headers=headers, json=body, timeout=CONFIG["TIMEOUT"])
            
            if resp.status_code == 200:
                for item in resp.json().get('results', []):
                    df = pd.DataFrame(item['data'])
                    if not df.empty:
                        df['period'] = pd.to_datetime(df['period'])
                        df.set_index('period', inplace=True)
                        all_data[item['title']] = df['ratio']
            else:
                logger.warning(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {resp.status_code}")
            
            time.sleep(0.3)
        
        return pd.DataFrame(all_data) if all_data else pd.DataFrame()
    except Exception as e:
        logger.error(f"ë„¤ì´ë²„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        return pd.DataFrame()


def get_google_trend_apify(candidates, api_key, status_container=None):
    """Apify Google Trends Scraper"""
    if not api_key:
        return pd.DataFrame()
    
    try:
        run_url = "https://api.apify.com/v2/acts/emastra~google-trends-scraper/runs?token=" + api_key
        run_input = {
            "searchTerms": candidates,
            "timeRange": "today 6-m",
            "geo": "KR",
            "isMultiple": True,
            "skipDebugScreen": True,
            "maxItems": 1
        }
        
        resp = requests.post(run_url, json=run_input, timeout=CONFIG["TIMEOUT"])
        
        if resp.status_code == 201:
            run_data = resp.json()
            run_id = run_data.get("data", {}).get("id")
            
            if run_id:
                for i in range(45):
                    time.sleep(2)
                    if status_container:
                        status_container.info(f"ğŸ“Š êµ¬ê¸€ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì¤‘... ({i*2}/90ì´ˆ)")
                    
                    status_url = f"https://api.apify.com/v2/actor-runs/{run_id}?token={api_key}"
                    status_resp = requests.get(status_url, timeout=10)
                    
                    if status_resp.status_code == 200:
                        status = status_resp.json().get("data", {}).get("status")
                        if status == "SUCCEEDED":
                            dataset_id = status_resp.json().get("data", {}).get("defaultDatasetId")
                            if dataset_id:
                                items_url = f"https://api.apify.com/v2/datasets/{dataset_id}/items?token={api_key}"
                                items_resp = requests.get(items_url, timeout=10)
                                
                                if items_resp.status_code == 200:
                                    items = items_resp.json()
                                    all_data = {}
                                    
                                    for item in items:
                                        timeline = item.get("interestOverTime", {}).get("timelineData", [])
                                        
                                        if timeline:
                                            for idx, name in enumerate(candidates):
                                                dates, values = [], []
                                                
                                                for point in timeline:
                                                    try:
                                                        ts = point.get("time")
                                                        if not ts:
                                                            continue
                                                        
                                                        dt = datetime.fromtimestamp(int(ts))
                                                        val_list = point.get("value", [])
                                                        
                                                        val = val_list[idx] if idx < len(val_list) else None
                                                        if val is not None:
                                                            dates.append(dt)
                                                            values.append(val)
                                                    except Exception as e:
                                                        logger.warning(f"ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
                                                        continue
                                                
                                                if dates and values:
                                                    all_data[name] = pd.Series(values, index=dates)
                                    
                                    if all_data:
                                        df = pd.DataFrame(all_data)
                                        df.index = pd.to_datetime(df.index)
                                        return df.sort_index()
                            break
                        elif status in ["FAILED", "ABORTED", "TIMED-OUT"]:
                            break
    except Exception as e:
        logger.error(f"êµ¬ê¸€ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    
    return pd.DataFrame()


def get_news_trend(candidates):
    """ë‰´ìŠ¤ ì–¸ê¸‰ëŸ‰ ê¸°ë°˜ íŠ¸ë Œë“œ"""
    ddgs = DDGS()
    end_date = datetime.now()
    start_date = end_date - timedelta(days=CONFIG["TREND_DAYS"])
    all_counts = {}
    
    for name in candidates:
        try:
            time.sleep(random.uniform(0.5, 1.0))
            news = ddgs.news(f'"{name}"', region="kr-kr", safesearch="off", max_results=100)
            
            if news:
                date_counts = {}
                for article in news:
                    date_str = article.get('date', '')
                    try:
                        if date_str:
                            dt = pd.to_datetime(date_str).date()
                            if start_date.date() <= dt <= end_date.date():
                                week_start = dt - timedelta(days=dt.weekday())
                                date_counts[week_start] = date_counts.get(week_start, 0) + 1
                    except:
                        continue
                
                if date_counts:
                    all_counts[name] = date_counts
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            continue
    
    global_max = max([max(c.values()) for c in all_counts.values() if c], default=1)
    trend_data = {}
    
    for name, date_counts in all_counts.items():
        normalized = {k: (v / global_max) * 100 for k, v in date_counts.items()}
        trend_data[name] = normalized
    
    if trend_data:
        all_dates = set()
        current = start_date.date()
        while current <= end_date.date():
            week_start = current - timedelta(days=current.weekday())
            all_dates.add(week_start)
            current += timedelta(days=7)
        
        for data in trend_data.values():
            all_dates.update(data.keys())
        
        if all_dates:
            df_data = {name: [data.get(d, 0) for d in sorted(all_dates)] for name, data in trend_data.items()}
            df = pd.DataFrame(df_data, index=sorted(all_dates))
            df.index = pd.to_datetime(df.index)
            return df
    
    return pd.DataFrame()


def get_all_trends(candidates, keys, status_container):
    """ëª¨ë“  íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘"""
    results = {"naver": pd.DataFrame(), "google": pd.DataFrame()}
    
    status_container.info("ğŸ“Š ë„¤ì´ë²„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì¤‘...")
    results["naver"] = get_naver_trend(candidates, keys["naver_id"], keys["naver_secret"])
    
    if keys.get("apify"):
        results["google"] = get_google_trend_apify(candidates, keys["apify"], status_container)
    
    if results["google"].empty:
        status_container.info("ğŸ“Š ë‰´ìŠ¤ ì–¸ê¸‰ëŸ‰ ìˆ˜ì§‘ ì¤‘...")
        results["google"] = get_news_trend(candidates)
    
    return results


def predict_future(df, days=30):
    """ë¯¸ë˜ íŠ¸ë Œë“œ ì˜ˆì¸¡"""
    if df.empty:
        return pd.DataFrame()
    
    future_df = pd.DataFrame()
    last_date = df.index[-1]
    future_dates = [last_date + timedelta(days=i) for i in range(1, days + 1)]
    
    for col in df.columns:
        series = df[col].dropna()
        if len(series) < 5:
            continue
        
        recent = series.tail(min(30, len(series)))
        x, y = np.arange(len(recent)), recent.values
        
        if len(x) > 1:
            try:
                z = np.polyfit(x, y, 1)
                p = np.poly1d(z)
                predictions = p(np.arange(len(recent), len(recent) + days))
                min_value = max(recent.iloc[-1] * 0.5, 1)
                future_df[col] = np.clip(predictions, min_value, 100)
            except Exception as e:
                logger.warning(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
    
    if not future_df.empty:
        future_df.index = future_dates
    
    return future_df


def contains_name(text, name):
    """í…ìŠ¤íŠ¸ì— ì´ë¦„ í¬í•¨ ì—¬ë¶€"""
    if not text or not name:
        return False
    return name in text or (len(name) >= 2 and name[1:] in text)


def collect_all_data(keyword, election_name):
    """í›„ë³´ì ê´€ë ¨ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘"""
    ddgs = DDGS()
    collected = {
        "news": {"text": [], "links": []},
        "sns": {"text": [], "links": []},
        "community": {"text": [], "links": []},
        "wiki": {"text": [], "links": []},
        "youtube": {"text": [], "links": []}
    }
    
    # ë‰´ìŠ¤
    try:
        time.sleep(random.uniform(0.5, 1.0))
        news = ddgs.news(f'"{keyword}" 2025 OR 2026', region="kr-kr", safesearch="off", max_results=10)
        if not news or len(news) < 3:
            news = ddgs.news(f'"{keyword}" {election_name}', region="kr-kr", safesearch="off", max_results=10)
        
        for r in (news or []):
            title, body, date_str = r.get('title', ''), r.get('body', ''), r.get('date', '')
            if contains_name(title, keyword) or contains_name(body, keyword):
                collected["news"]["text"].append(f"[{date_str}] {title}: {body[:200]}")
                collected["news"]["links"].append({
                    "title": title[:50],
                    "url": r.get('url', '#'),
                    "source": r.get('source', ''),
                    "date": date_str,
                    "body": body[:300]
                })
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    
    # ìœ„í‚¤
    try:
        time.sleep(random.uniform(0.5, 1.0))
        profile = ddgs.text(f'"{keyword}" ì •ë‹¹ ì†Œì† í˜„ì¬ 2025', region="kr-kr", safesearch="off", max_results=5)
        for r in (profile or []):
            title, body = r.get('title', ''), r.get('body', '')
            if contains_name(title, keyword) or contains_name(body, keyword):
                collected["wiki"]["text"].insert(0, f"{title}: {body[:250]}")
                collected["wiki"]["links"].insert(0, {"title": title[:50], "url": r.get('href', '#')})
    except Exception as e:
        logger.error(f"í”„ë¡œí•„ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    
    try:
        time.sleep(random.uniform(0.5, 1.0))
        wiki = ddgs.text(f'"{keyword}" (site:namu.wiki OR site:ko.wikipedia.org)', region="kr-kr", safesearch="off", max_results=5)
        for r in (wiki or []):
            title, body = r.get('title', ''), r.get('body', '')
            if contains_name(title, keyword) or contains_name(body, keyword):
                collected["wiki"]["text"].append(f"{title}: {body[:300]}")
                collected["wiki"]["links"].append({"title": title[:50], "url": r.get('href', '#')})
    except Exception as e:
        logger.error(f"ìœ„í‚¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    
    # SNS
    try:
        time.sleep(random.uniform(0.5, 1.0))
        sns = ddgs.text(f'"{keyword}" (site:blog.naver.com OR site:cafe.naver.com OR site:tistory.com)', region="kr-kr", safesearch="off", max_results=15)
        for r in (sns or []):
            title, body = r.get('title', ''), r.get('body', '')
            if contains_name(title, keyword) or contains_name(body, keyword):
                url = r.get('href', '')
                source = "ë„¤ì´ë²„ë¸”ë¡œê·¸" if 'blog.naver' in url else "ë„¤ì´ë²„ì¹´í˜" if 'cafe.naver' in url else "í‹°ìŠ¤í† ë¦¬" if 'tistory' in url else "SNS"
                collected["sns"]["text"].append(f"[{source}] {title}: {body[:150]}")
                collected["sns"]["links"].append({"title": title[:50], "url": url, "source": source})
    except Exception as e:
        logger.error(f"SNS ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    
    # ì»¤ë®¤ë‹ˆí‹°
    try:
        time.sleep(random.uniform(0.5, 1.0))
        community = ddgs.text(f'"{keyword}" (site:dcinside.com OR site:clien.net)', region="kr-kr", safesearch="off", max_results=10)
        for r in (community or []):
            title, body = r.get('title', ''), r.get('body', '')
            if contains_name(title, keyword) or contains_name(body, keyword):
                url = r.get('href', '')
                source = "ë””ì‹œì¸ì‚¬ì´ë“œ" if 'dcinside' in url else "í´ë¦¬ì•™" if 'clien' in url else "ì»¤ë®¤ë‹ˆí‹°"
                collected["community"]["text"].append(f"[{source}] {title}: {body[:150]}")
                collected["community"]["links"].append({"title": title[:50], "url": url, "source": source})
    except Exception as e:
        logger.error(f"ì»¤ë®¤ë‹ˆí‹° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    
    # ìœ íŠœë¸Œ
    try:
        time.sleep(random.uniform(0.5, 1.0))
        videos = ddgs.videos(f'"{keyword}" {election_name}', region="kr-kr", safesearch="off", max_results=5)
        for r in (videos or []):
            title = r.get('title', '')
            if contains_name(title, keyword):
                collected["youtube"]["text"].append(title)
                collected["youtube"]["links"].append({"title": title[:50], "url": r.get('content', '#')})
    except Exception as e:
        logger.error(f"ìœ íŠœë¸Œ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
    
    return collected


def analyze_candidate(model, name, collected_data, trend_info):
    """í›„ë³´ì AI ë¶„ì„"""
    news_cnt = len(collected_data.get("news", {}).get("links", []))
    sns_cnt = len(collected_data.get("sns", {}).get("links", []))
    community_cnt = len(collected_data.get("community", {}).get("links", []))
    wiki_cnt = len(collected_data.get("wiki", {}).get("links", []))
    
    def build_career_timeline(news_links):
        """ë‰´ìŠ¤ ë°œí–‰ì¼ ê¸°ë°˜ ê²½ë ¥ íƒ€ì„ë¼ì¸"""
        timeline = []
        
        for link in news_links:
            date_str = link.get('date', '')
            if not date_str:
                continue
            
            try:
                article_date = pd.to_datetime(date_str)
                content = f"{link.get('title', '')} {link.get('body', '')}"
                
                if article_date < pd.Timestamp('2013-02-25'):
                    government = "ì´ëª…ë°• ì •ë¶€"
                elif article_date < pd.Timestamp('2017-05-10'):
                    government = "ë°•ê·¼í˜œ ì •ë¶€"
                elif article_date < pd.Timestamp('2022-05-10'):
                    government = "ë¬¸ì¬ì¸ ì •ë¶€"
                elif article_date < pd.Timestamp('2025-05-10'):
                    government = "ìœ¤ì„ì—´ ì •ë¶€"
                else:
                    government = "í˜„ ì •ë¶€"
                
                careers = []
                
                if "ì²­ì™€ëŒ€" in content or "ëŒ€í†µë ¹ì‹¤" in content:
                    if "ì²­ë…„ìœ„ì›ì¥" in content:
                        careers.append(f"{government} ì²­ë…„ìœ„ì›ì¥")
                    elif "ìˆ˜ì„" in content:
                        careers.append(f"{government} ìˆ˜ì„")
                    else:
                        careers.append(f"{government} ì²­ì™€ëŒ€" if government in ["ì´ëª…ë°• ì •ë¶€", "ë°•ê·¼í˜œ ì •ë¶€", "ë¬¸ì¬ì¸ ì •ë¶€"] else f"{government} ëŒ€í†µë ¹ì‹¤")
                
                if "êµ­íšŒì˜ì›" in content:
                    for term in ["22ëŒ€", "21ëŒ€", "20ëŒ€", "19ëŒ€", "18ëŒ€"]:
                        if term in content:
                            careers.append(f"{term} êµ­íšŒì˜ì›")
                            break
                
                if "ë„ì§€ì‚¬" in content and "í›„ë³´" not in content:
                    careers.append("ê´‘ì—­ë‹¨ì²´ì¥")
                if "ì¥ê´€" in content:
                    careers.append("ì¥ê´€")
                
                for career in careers:
                    timeline.append({"date": article_date, "career": career})
            except:
                continue
        
        timeline.sort(key=lambda x: x['date'], reverse=True)
        seen = set()
        unique = []
        for item in timeline:
            if item['career'] not in seen:
                seen.add(item['career'])
                unique.append(item['career'])
        
        return unique
    
    career_timeline = build_career_timeline(collected_data.get("news", {}).get("links", []))
    
    all_text = (
        collected_data.get("news", {}).get("text", [])[:5] + 
        collected_data.get("wiki", {}).get("text", [])[:5] +
        collected_data.get("sns", {}).get("text", [])[:3] + 
        collected_data.get("community", {}).get("text", [])[:2]
    )
    raw_text = "\n".join(all_text)
    
    party_patterns = {
        "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹": ["ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹", "ë¯¼ì£¼ë‹¹ ì†Œì†", "ë¯¼ì£¼ë‹¹ ê³µì²œ"],
        "êµ­ë¯¼ì˜í˜": ["êµ­ë¯¼ì˜í˜", "êµ­ë¯¼ì˜í˜ ì†Œì†", "êµ­ë¯¼ì˜í˜ ê³µì²œ"],
        "ê°œí˜ì‹ ë‹¹": ["ê°œí˜ì‹ ë‹¹"],
        "ë¬´ì†Œì†": ["ë¬´ì†Œì†"]
    }
    
    detected_party = "ì •ë³´ ë¶€ì¡±"
    max_matches = 0
    
    for party, patterns in party_patterns.items():
        match_count = sum(raw_text.count(p) for p in patterns)
        if match_count > max_matches:
            max_matches = match_count
            detected_party = party
    
    poll_est = 50
    try:
        if trend_info != "ë°ì´í„° ì—†ìŒ":
            match = re.search(r'í˜„ì¬\s*([\d.]+)', trend_info)
            if match:
                poll_est = min(int(float(match.group(1)) * 8), 100)
    except:
        pass
    
    if not raw_text or len(raw_text) < 30:
        return {
            "name": name,
            "party": "ë°ì´í„° ë¶€ì¡±",
            "current_role": "ì •ë³´ ë¶€ì¡±",
            "past_career": ", ".join(career_timeline[:3]) if career_timeline else "ì •ë³´ ë¶€ì¡±",
            "poll_est": poll_est,
            "analysis": "ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.",
            "sns_sentiment": "ë¶„ì„ ë¶ˆê°€",
            "keywords": [name]
        }
    
    career_info = ", ".join(career_timeline[:5]) if career_timeline else "ê²½ë ¥ ì •ë³´ ì—†ìŒ"
    
    prompt = f'''ë‹¤ìŒì€ {name} í›„ë³´ ì •ë³´ì…ë‹ˆë‹¤.

íŠ¸ë Œë“œ: {trend_info}
ê²½ë ¥: {career_info}

ë°ì´í„°:
{raw_text[:2000]}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"name":"{name}","party":"ì •ë‹¹ëª…","current_role":"í˜„ì¬ ì§í•¨","past_career":"ì£¼ìš” ê²½ë ¥","poll_est":{poll_est},"analysis":"ë¶„ì„","sns_sentiment":"SNS ì—¬ë¡ ","keywords":["í‚¤ì›Œë“œ"]}}'''

    for attempt in range(3):
        try:
            resp = model.generate_content(prompt)
            result = clean_json(resp.text)
            if result and 'name' in result:
                if isinstance(result.get('poll_est'), str):
                    nums = re.findall(r'\d+', str(result['poll_est']))
                    result['poll_est'] = int(nums[0]) if nums else poll_est
                
                ai_party = result.get('party', '')
                if not ai_party or ai_party in ['ì •ë³´ ì—†ìŒ', '']:
                    result['party'] = detected_party
                
                if not result.get('past_career'):
                    result['past_career'] = ", ".join(career_timeline[:3]) if career_timeline else "ì •ë³´ ë¶€ì¡±"
                
                return result
        except Exception as e:
            logger.warning(f"AI ì‹¤íŒ¨ ({attempt+1}/3): {str(e)}")
            time.sleep(3)
    
    return {
        "name": name,
        "party": detected_party,
        "current_role": "ì¶©ë¶ì§€ì‚¬ í›„ë³´",
        "past_career": ", ".join(career_timeline[:3]) if career_timeline else "ì •ë³´ ë¶€ì¡±",
        "poll_est": poll_est,
        "analysis": f"{name} í›„ë³´: ë‰´ìŠ¤ {news_cnt}ê±´ ë¶„ì„",
        "sns_sentiment": f"SNS/ì»¤ë®¤ë‹ˆí‹° {sns_cnt + community_cnt}ê±´",
        "keywords": [name, detected_party] if detected_party != "ì •ë³´ ë¶€ì¡±" else [name]
    }


def create_trend_chart(trends, pred_trends, candidates, google_source):
    """íŠ¸ë Œë“œ ì°¨íŠ¸ ìƒì„±"""
    colors = CONFIG["COLORS"]
    naver_df = trends.get("naver", pd.DataFrame())
    google_df = trends.get("google", pd.DataFrame())
    pred_naver = pred_trends.get("naver", pd.DataFrame())
    pred_google = pred_trends.get("google", pd.DataFrame())
    
    fig = make_subplots(rows=1, cols=2, subplot_titles=("ğŸŸ¢ ë„¤ì´ë²„ íŠ¸ë Œë“œ", f"ğŸ”µ {google_source}"), horizontal_spacing=0.08)
    
    if not naver_df.empty:
        for idx, col in enumerate(candidates):
            if col in naver_df.columns:
                color = colors[idx % len(colors)]
                fig.add_trace(go.Scatter(x=naver_df.index, y=naver_df[col], mode='lines', name=col,
                              line=dict(color=color, width=2), legendgroup=col), row=1, col=1)
                if not pred_naver.empty and col in pred_naver.columns:
                    pred_x = [naver_df.index[-1]] + list(pred_naver.index)
                    pred_y = [naver_df[col].iloc[-1]] + list(pred_naver[col])
                    fig.add_trace(go.Scatter(x=pred_x, y=pred_y, mode='lines',
                                  line=dict(color=color, width=2, dash='dot'), legendgroup=col, showlegend=False), row=1, col=1)
    
    if not google_df.empty:
        for idx, col in enumerate(candidates):
            if col in google_df.columns:
                color = colors[idx % len(colors)]
                fig.add_trace(go.Scatter(x=google_df.index, y=google_df[col], mode='lines',
                              line=dict(color=color, width=2), legendgroup=col, showlegend=False), row=1, col=2)
                if not pred_google.empty and col in pred_google.columns:
                    pred_x = [google_df.index[-1]] + list(pred_google.index)
                    pred_y = [google_df[col].iloc[-1]] + list(pred_google[col])
                    fig.add_trace(go.Scatter(x=pred_x, y=pred_y, mode='lines',
                                  line=dict(color=color, width=2, dash='dot'), legendgroup=col, showlegend=False), row=1, col=2)
    
    fig.update_layout(height=400, template="plotly_dark", paper_bgcolor='#0f172a', plot_bgcolor='#1e293b',
                      font=dict(color='#e2e8f0'), legend=dict(orientation="h", y=-0.25, x=0.5, xanchor="center"),
                      margin=dict(l=50, r=50, t=50, b=80))
    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#334155')
    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='#334155')
    return fig


def get_party_color(party):
    """ì •ë‹¹ë³„ ìƒ‰ìƒ"""
    if not party:
        return "#4b5563"
    if "ë¯¼ì£¼" in party:
        return "#1d4ed8"
    elif "êµ­ë¯¼ì˜í˜" in party:
        return "#b91c1c"
    elif "ê°œí˜" in party:
        return "#f97316"
    return "#4b5563"


def render_candidate_card(c, collected, trends, pred_trends, google_label):
    """í›„ë³´ì ì¹´ë“œ ë Œë”ë§"""
    name = c.get('name', 'ë¯¸ìƒ')
    party = c.get('party', 'ì •ë³´ ì—†ìŒ')
    party_color = get_party_color(party)
    
    naver_df = trends.get("naver", pd.DataFrame())
    google_df = trends.get("google", pd.DataFrame())
    pred_naver = pred_trends.get("naver", pd.DataFrame())
    pred_google = pred_trends.get("google", pd.DataFrame())
    
    badge = ""
    curr_val = safe_get_last_value(naver_df, name)
    fut_val = safe_get_last_value(pred_naver, name)
    
    if curr_val is not None and fut_val is not None:
        if fut_val > curr_val * 1.1:
            badge = "ğŸ“ˆ ìƒìŠ¹ì˜ˆì¸¡"
        elif fut_val < curr_val * 0.9:
            badge = "ğŸ“‰ í•˜ë½ì˜ˆì¸¡"
        else:
            badge = "â¡ï¸ ìœ ì§€"
    
    with st.container():
        col1, col2 = st.columns([3, 1])
        with col1:
            badge_html = f"<span style='background:#2563eb;color:white;padding:3px 8px;border-radius:10px;font-size:0.4em;margin-left:8px;'>{badge}</span>" if badge else ""
            st.markdown(f"### {name} <span style='background:{party_color};color:white;padding:4px 12px;border-radius:6px;font-size:0.5em;margin-left:10px;'>{party}</span>{badge_html}", unsafe_allow_html=True)
        with col2:
            st.metric("í™”ì œì„±", c.get('poll_est', 0))
        
        tc1, tc2, tc3, tc4 = st.columns(4)
        with tc1:
            val = safe_get_last_value(naver_df, name)
            st.metric("ë„¤ì´ë²„ í˜„ì¬", f"{val:.1f}" if val is not None else "-")
        with tc2:
            val = safe_get_last_value(pred_naver, name)
            st.metric("ë„¤ì´ë²„ ì˜ˆì¸¡", f"{val:.1f}" if val is not None else "-")
        with tc3:
            val = safe_get_last_value(google_df, name)
            st.metric(f"{google_label} í˜„ì¬", f"{val:.1f}" if val is not None else "-")
        with tc4:
            val = safe_get_last_value(pred_google, name)
            st.metric(f"{google_label} ì˜ˆì¸¡", f"{val:.1f}" if val is not None else "-")
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**ğŸ”µ í˜„ì¬ ì§í•¨**")
            st.info(c.get('current_role', 'ì •ë³´ ì—†ìŒ'))
        with col2:
            st.markdown("**âšª ì£¼ìš” ê²½ë ¥**")
            st.info(c.get('past_career', 'ì •ë³´ ì—†ìŒ'))
        
        st.markdown("**ğŸ“° ë‰´ìŠ¤ ë¶„ì„**")
        st.write(c.get('analysis', ''))
        
        st.markdown("**ğŸ’¬ SNS ì—¬ë¡ **")
        sns_sent = str(c.get('sns_sentiment', ''))
        if "ê¸ì •" in sns_sent:
            st.success(f"ğŸŸ¢ {sns_sent}")
        elif "ë¶€ì •" in sns_sent:
            st.error(f"ğŸ”´ {sns_sent}")
        else:
            st.warning(f"ğŸŸ¡ {sns_sent}")
        
        keywords = c.get('keywords', [])
        if keywords:
            st.markdown(" ".join([f"`#{k}`" for k in keywords[:5]]))
        
        st.markdown("**ğŸ“š ë°ì´í„° ì¶œì²˜**")
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            f"ğŸ“° ë‰´ìŠ¤({len(collected.get('news',{}).get('links',[]))})",
            f"ğŸ’¬ SNS({len(collected.get('sns',{}).get('links',[]))})",
            f"ğŸ‘¥ ì»¤ë®¤ë‹ˆí‹°({len(collected.get('community',{}).get('links',[]))})",
            f"ğŸ“– ìœ„í‚¤({len(collected.get('wiki',{}).get('links',[]))})",
            f"ğŸ“º ìœ íŠœë¸Œ({len(collected.get('youtube',{}).get('links',[]))})"
        ])
        
        with tab1:
            for l in collected.get("news", {}).get("links", []):
                st.markdown(f"- [{l['title']}]({l['url']})")
        with tab2:
            for l in collected.get("sns", {}).get("links", []):
                st.markdown(f"- [{l['title']}]({l['url']})")
        with tab3:
            for l in collected.get("community", {}).get("links", []):
                st.markdown(f"- [{l['title']}]({l['url']})")
        with tab4:
            for l in collected.get("wiki", {}).get("links", []):
                st.markdown(f"- [{l['title']}]({l['url']})")
        with tab5:
            for l in collected.get("youtube", {}).get("links", []):
                st.markdown(f"- [{l['title']}]({l['url']})")
        
        st.divider()


def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    st.title("ğŸ›ï¸ ì„ ê±° ì „ëµ ì¸ì‚¬ì´íŠ¸: ì˜ˆì¸¡ê³¼ ì „ë§")
    st.caption("ë„¤ì´ë²„ + êµ¬ê¸€(Apify) íŠ¸ë Œë“œ ì¢…í•© ë¶„ì„")
    
    keys = load_api_keys()
    
    with st.sidebar:
        st.success("âœ… ì‹œìŠ¤í…œ ì •ìƒ ê°€ë™")
        
        if keys.get("apify"):
            st.info("ğŸ”µ Apify ì—°ê²°ë¨")
        else:
            st.warning("âš ï¸ Apify ë¯¸ì„¤ì •")
        
        st.divider()
        
        election = st.text_input("ë¶„ì„ ëŒ€ìƒ ì„ ê±°", value="2026ë…„ ì¶©ì²­ë¶ë„ì§€ì‚¬ ì„ ê±°")
        st.markdown("**í›„ë³´ì ëª©ë¡**")
        cands_txt = st.text_area("", value="ì‹ ìš©í•œ\në…¸ì˜ë¯¼\nì†¡ê¸°ì„­", height=150, label_visibility="collapsed")
        
        cands = [c.strip() for c in cands_txt.split('\n') if c.strip()]
        st.caption(f"ë“±ë¡: {len(cands)}ëª…")
        
        is_valid, msg = validate_candidates(cands)
        if not is_valid:
            st.error(msg)
        
        start = st.button("ğŸš€ ì¢…í•© ë¶„ì„ ì‹¤í–‰", type="primary", use_container_width=True, disabled=not is_valid)

    if start:
        model = genai.GenerativeModel(get_best_model(keys["gemini"]))
        status = st.empty()
        progress = st.progress(0)
        
        try:
            trends = get_all_trends(cands, keys, status)
            progress.progress(0.25)
            
            google_source = "êµ¬ê¸€ íŠ¸ë Œë“œ" if keys.get("apify") and not trends["google"].empty else "ë‰´ìŠ¤ ì–¸ê¸‰ëŸ‰"
            google_label = "êµ¬ê¸€" if "êµ¬ê¸€" in google_source else "ë‰´ìŠ¤"
            
            status.info("ğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ê³„ì‚° ì¤‘...")
            pred_trends = {
                "naver": predict_future(trends["naver"], days=CONFIG["PREDICTION_DAYS"]),
                "google": predict_future(trends["google"], days=CONFIG["PREDICTION_DAYS"])
            }
            progress.progress(0.3)
            
            results, all_collected = [], {}
            for i, name in enumerate(cands):
                status.info(f"âš¡ [{i+1}/{len(cands)}] {name} ë¶„ì„ ì¤‘...")
                collected = collect_all_data(name, election)
                all_collected[name] = collected
                
                trend_info = "ë°ì´í„° ì—†ìŒ"
                curr_val = safe_get_last_value(trends.get("naver", pd.DataFrame()), name)
                fut_val = safe_get_last_value(pred_trends.get("naver", pd.DataFrame()), name)
                
                if curr_val is not None:
                    fut_val = fut_val if fut_val is not None else curr_val
                    trend_info = f"í˜„ì¬ {curr_val:.1f}, ì˜ˆì¸¡ {fut_val:.1f}"
                
                results.append(analyze_candidate(model, name, collected, trend_info))
                progress.progress(0.3 + (0.65 * (i + 1) / len(cands)))
                time.sleep(2)
            
            status.empty()
            progress.empty()
            st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
            
            st.subheader("ğŸ“ˆ íŠ¸ë Œë“œ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜")
            st.plotly_chart(create_trend_chart(trends, pred_trends, cands, google_source), use_container_width=True)
            
            st.markdown("**ğŸ“Š íŠ¸ë Œë“œ ìˆ˜ì¹˜ ìš”ì•½**")
            naver_df = trends.get("naver", pd.DataFrame())
            google_df = trends.get("google", pd.DataFrame())
            pred_naver = pred_trends.get("naver", pd.DataFrame())
            pred_google = pred_trends.get("google", pd.DataFrame())
            
            summary = []
            for name in cands:
                row = {"í›„ë³´": name}
                row["ë„¤ì´ë²„ í˜„ì¬"] = f"{safe_get_last_value(naver_df, name):.1f}" if safe_get_last_value(naver_df, name) is not None else "-"
                row["ë„¤ì´ë²„ ì˜ˆì¸¡"] = f"{safe_get_last_value(pred_naver, name):.1f}" if safe_get_last_value(pred_naver, name) is not None else "-"
                row[f"{google_label} í˜„ì¬"] = f"{safe_get_last_value(google_df, name):.1f}" if safe_get_last_value(google_df, name) is not None else "-"
                row[f"{google_label} ì˜ˆì¸¡"] = f"{safe_get_last_value(pred_google, name):.1f}" if safe_get_last_value(pred_google, name) is not None else "-"
                summary.append(row)
            
            st.dataframe(pd.DataFrame(summary), use_container_width=True, hide_index=True)
            st.divider()
            
            st.subheader("ğŸ“‹ í›„ë³´ìë³„ ì‹¬ì¸µ ë¦¬í¬íŠ¸")
            for c in results:
                render_candidate_card(c, all_collected.get(c['name'], {}), trends, pred_trends, google_label)
        
        except Exception as e:
            status.empty()
            progress.empty()
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"ë©”ì¸ ì˜¤ë¥˜: {str(e)}", exc_info=True)


if __name__ == "__main__":
    main()
